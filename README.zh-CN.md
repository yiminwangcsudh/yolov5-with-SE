# yolov5-with-SE

　 　 注意力机制源于对人类视觉的研究。在认知科学中，由于信息处理的瓶颈，人类选择性地关注所有信息的一部分而忽略其他可见信息。之所以能实现这种能力，是因为人类视网膜的不同部位具有不同的信息处理能力，即不同部位的敏锐度不同，其中人类视网膜的中央凹的敏锐度最高。为了合理利用有限的视觉信息处理资源，人类需要选择视觉区域的特定部分，然后对其进行聚焦。例如，人们在使用电脑屏幕看电影时，会关注和处理电脑屏幕范围内的视觉，而电脑屏幕之外的视觉，如键盘、电脑背景等，会被忽视。

　 　 在神经网络中引入attention机制的方式有很多种，以卷积神经网络为例，可以在空间维度增加attention机制的引入，也可以在channel维度增加attention机制，当然有还有混合维度，即在空间维度和通道维度同时加入注意力机制。![image](https://user-images.githubusercontent.com/120677884/207951728-b85f94dd-e95f-42c9-a681-5b3293f15b8d.png)　 　 　 　 　 　 　 　 　 　 　 　 SE构建块的结构

　 　 Squeeze：将一个通道上的整个空间特征编码成一个全局特征，使用全局平均池化将每个通道的二维特征（H×W）压缩成一个实数。

　 　 Excitation：为每个特征通道动态生成一个权重值。它使用两个全连接层组成瓶颈结构来对通道之间的相关性进行建模，并输出与输入特征相同数量的权重值。

　 　 Scale：将激励学习到的归一化权重加权到每个通道的特征上。
